{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 384\n",
    "BLOCK_SIZE = 512\n",
    "HEAD_SIZE = 6\n",
    "EPOCHS = 5000\n",
    "EPOCHS_VAL = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./war_and_peace.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лев Николаевич Толстой\n",
      "Война и мир. Книга 1\n",
      "\n",
      "Война и мир – 1\n",
      "\n",
      "Аннотация \n",
      "\n",
      "Роман Льва Толстого «Война и мир» лежит в основании величественного здания русской классической литературы. С непревзойденным мастерством Толстой воссоздал великую духом Россию – образы этой «книги на все времена» и сейчас пленяют свежестью чувств и щедростью души, искренностью страстей, силой и чистотой убеждений.\n",
      "В книгу вошли первый и второй тома романа.\n",
      "\n",
      "Лев Николаевич Толстой\n",
      "ВОЙНА И МИР\n",
      "\n",
      "Том 1\n",
      "\n",
      "ЧАСТЬ ПЕРВАЯ\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "– Еh bien, mon prince. Genes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous previens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocites de cet Antichrist (ma parole, j'y crois) – je ne vous connais plus, vous n'etes plus mon ami, vous n'etes plus мой верный раб, comme vous dites. [Ну, что, князь, Генуа и Лукка стали не больше, как поместьями фамилии Бонапарте. Нет, \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !&'()*,.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXZ[]`abcdefghijklmnopqrstuvwxyz«»АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё–“„…\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(''.join(chars))\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94, 125, 117, 111, 114, 127, 8, 1, 119, 109, 119, 1, 113, 114, 120, 109, 22]\n",
      "Привет, как дела?\n"
     ]
    }
   ],
   "source": [
    "print(encode('Привет, как дела?'))\n",
    "print(decode(encode('Привет, как дела?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1466823])\n",
      "tensor([ 90, 114, 111,   1,  92, 117, 119, 123, 120, 109, 114, 111, 117, 132,\n",
      "          1,  97, 123, 120, 126, 127, 123, 118,   0,  81, 123, 118, 122, 109,\n",
      "          1, 117,   1, 121, 117, 125,   9,   1,  89, 122, 117, 112, 109,   1,\n",
      "         11,   0,   0,  81, 123, 118, 122, 109,   1, 117,   1, 121, 117, 125,\n",
      "          1, 142,   1,  11,   0,   0,  79, 122, 122, 123, 127, 109, 131, 117,\n",
      "        140,   1,   0,   0,  95, 123, 121, 109, 122,   1,  90, 137, 111, 109,\n",
      "          1,  97, 123, 120, 126, 127, 123, 112, 123,   1,  77,  81, 123, 118,\n",
      "        122, 109,   1, 117,   1, 121, 117, 125,  78,   1, 120, 114, 115, 117,\n",
      "        127,   1, 111,   1, 123, 126, 122, 123, 111, 109, 122, 117, 117,   1,\n",
      "        111, 114, 120, 117, 132, 114, 126, 127, 111, 114, 122, 122, 123, 112,\n",
      "        123,   1, 116, 113, 109, 122, 117, 140,   1, 125, 128, 126, 126, 119,\n",
      "        123, 118,   1, 119, 120, 109, 126, 126, 117, 132, 114, 126, 119, 123,\n",
      "        118,   1, 120, 117, 127, 114, 125, 109, 127, 128, 125, 136,   9,   1,\n",
      "         96,   1, 122, 114, 124, 125, 114, 111, 116, 123, 118, 113, 114, 122,\n",
      "        122, 136, 121,   1, 121, 109, 126, 127, 114, 125, 126, 127, 111, 123,\n",
      "        121,   1,  97, 123, 120, 126, 127, 123, 118,   1, 111, 123, 126, 126,\n",
      "        123, 116, 113, 109, 120,   1, 111, 114, 120, 117, 119, 128, 139,   1,\n",
      "        113, 128, 130, 123, 121,   1,  95, 123, 126, 126, 117, 139,   1, 142,\n",
      "          1, 123, 110, 125, 109, 116, 136,   1, 138, 127, 123, 118,   1,  77,\n",
      "        119, 122, 117, 112, 117,   1, 122, 109,   1, 111, 126, 114,   1, 111,\n",
      "        125, 114, 121, 114, 122, 109,  78,   1, 117,   1, 126, 114, 118, 132,\n",
      "        109, 126,   1, 124, 120, 114, 122, 140, 139, 127,   1, 126, 111, 114,\n",
      "        115, 114, 126, 127, 137, 139,   1, 132, 128, 111, 126, 127, 111,   1,\n",
      "        117,   1, 134, 114, 113, 125, 123, 126, 127, 137, 139,   1, 113, 128,\n",
      "        133, 117,   8,   1, 117, 126, 119, 125, 114, 122, 122, 123, 126, 127,\n",
      "        137, 139,   1, 126, 127, 125, 109, 126, 127, 114, 118,   8,   1, 126,\n",
      "        117, 120, 123, 118,   1, 117,   1, 132, 117, 126, 127, 123, 127, 123,\n",
      "        118,   1, 128, 110, 114, 115, 113, 114, 122, 117, 118,   9,   0,  81,\n",
      "          1, 119, 122, 117, 112, 128,   1, 111, 123, 133, 120, 117,   1, 124,\n",
      "        114, 125, 111, 136, 118,   1, 117,   1, 111, 127, 123, 125, 123, 118,\n",
      "          1, 127, 123, 121, 109,   1, 125, 123, 121, 109, 122, 109,   9,   0,\n",
      "          0,  90, 114, 111,   1,  92, 117, 119, 123, 120, 109, 114, 111, 117,\n",
      "        132,   1,  97, 123, 120, 126, 127, 123, 118,   0,  81,  93,  88,  92,\n",
      "         79,   1,  87,   1,  91,  87,  95,   0,   0,  97, 123, 121,   1,  11,\n",
      "          0,   0, 102,  79,  96,  97, 105,   1,  94,  84,  95,  81,  79, 108,\n",
      "          0,   0,   0,  31,   0,   0, 142,   1,  84,  58,   1,  52,  59,  55,\n",
      "         64,   8,   1,  63,  65,  64,   1,  66,  68,  59,  64,  53,  55,   9,\n",
      "          1,  29,  55,  64,  55,  69,   1,  55,  70,   1,  34,  71,  53,  67,\n",
      "         71,  55,  69,   1,  64,  55,   1,  69,  65,  64,  70,   1,  66,  62,\n",
      "         71,  69,   1,  67,  71,  55,   1,  54,  55,  69,   1,  51,  66,  51,\n",
      "         64,  51,  57,  55,  69,   8,   1,  54,  55,  69,   1, 124, 123, 121,\n",
      "        114, 126, 127, 137, 140,   8,   1,  54,  55,   1,  62,  51,   1,  56,\n",
      "         51,  63,  59,  62,  62,  55,   1,  24,  71,  65,  64,  51,  66,  51,\n",
      "         68,  70,  55,   9,   1,  36,  65,  64,   8,   1,  60,  55,   1,  72,\n",
      "         65,  71,  69,   1,  66,  68,  55,  72,  59,  55,  64,  69,   8,   1,\n",
      "         67,  71,  55,   1,  69,  59,   1,  72,  65,  71,  69,   1,  64,  55,\n",
      "          1,  63,  55,   1,  54,  59,  70,  55,  69,   1,  66,  51,  69,   8,\n",
      "          1,  67,  71,  55,   1,  64,  65,  71,  69,   1,  51,  72,  65,  64,\n",
      "         69,   1,  62,  51,   1,  57,  71,  55,  68,  68,  55,   8,   1,  69,\n",
      "         59,   1,  72,  65,  71,  69,   1,  72,  65,  71,  69,   1,  66,  55,\n",
      "         68,  63,  55,  70,  70,  55,  76,   1,  55,  64,  53,  65,  68,  55,\n",
      "          1,  54,  55,   1,  66,  51,  62,  62,  59,  55,  68,   1,  70,  65,\n",
      "         71,  70,  55,  69,   1,  62,  55,  69,   1,  59,  64,  56,  51,  63,\n",
      "         59,  55,  69,   8,   1,  70,  65,  71,  70,  55,  69,   1,  62,  55,\n",
      "         69,   1,  51,  70,  68,  65,  53,  59,  70,  55,  69,   1,  54,  55,\n",
      "          1,  53,  55,  70,   1,  23,  64,  70,  59,  53,  58,  68,  59,  69,\n",
      "         70,   1,   5,  63,  51,   1,  66,  51,  68,  65,  62,  55,   8,   1,\n",
      "         60,   4,  75,   1,  53,  68,  65,  59,  69,   6,   1, 142,   1,  60,\n",
      "         55,   1,  64,  55,   1,  72,  65,  71,  69,   1,  53,  65,  64,  64,\n",
      "         51,  59,  69,   1,  66,  62,  71,  69,   8,   1,  72,  65,  71,  69,\n",
      "          1,  64,   4,  55,  70,  55,  69,   1,  66,  62,  71,  69,   1,  63,\n",
      "         65,  64,   1,  51,  63,  59,   8,   1,  72,  65,  71,  69,   1,  64,\n",
      "          4,  55,  70,  55,  69,   1,  66,  62,  71,  69,   1, 121, 123, 118,\n",
      "          1, 111, 114, 125, 122, 136, 118,   1, 125, 109, 110,   8,   1,  53,\n",
      "         65,  63,  63,  55,   1,  72,  65,  71,  69,   1,  54,  59,  70,  55,\n",
      "         69,   9,   1,  48,  92, 128,   8,   1, 132, 127, 123,   8,   1, 119,\n",
      "        122, 140, 116, 137,   8,   1,  82, 114, 122, 128, 109,   1, 117,   1,\n",
      "         90, 128, 119, 119, 109,   1, 126, 127, 109, 120, 117,   1, 122, 114,\n",
      "          1, 110, 123, 120, 137, 133, 114,   8,   1, 119, 109, 119,   1, 124,\n",
      "        123, 121, 114, 126, 127, 137, 140, 121, 117,   1, 129, 109, 121, 117,\n",
      "        120, 117, 117,   1,  80, 123, 122, 109, 124, 109, 125, 127, 114,   9,\n",
      "          1,  92, 114, 127,   8,   1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(0.9*len(data))\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 90, 114, 111,   1,  92, 117, 119, 123, 120])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[ 57,  71,  55,  68,  68,  55,   8,   1],\n",
      "        [140,   1, 126,   1, 126, 111, 123, 117],\n",
      "        [137,   1, 124, 114, 126, 122, 117,   8],\n",
      "        [112, 123, 118,   1, 126,   1, 113, 114],\n",
      "        [133, 114, 122, 122, 136, 121,   1, 124],\n",
      "        [109, 111, 133, 109, 140, 126, 140,   1],\n",
      "        [116, 122, 109, 122, 117, 114,   8,   1],\n",
      "        [116, 140, 111,   1, 117, 116,   1, 133],\n",
      "        [113, 128, 139, 134, 114, 112, 123,   9],\n",
      "        [123,   1, 110, 136, 120, 123,   1, 114],\n",
      "        [109, 139,   8,   1, 109,   1, 127, 114],\n",
      "        [140, 120, 117,   1, 124, 125, 117, 126],\n",
      "        [  1, 116, 128, 110, 136,   8,   1, 133],\n",
      "        [114, 125, 109, 120,   8,   1, 132, 127],\n",
      "        [  8,   1, 119, 127, 123,   1, 120, 123],\n",
      "        [112, 123,   1, 113, 123, 110, 125, 109],\n",
      "        [128, 120, 136, 110, 119, 109,   9,   0],\n",
      "        [123, 126, 127, 137, 139,   9,   1,  93],\n",
      "        [109,   1, 126, 111, 123, 117, 121,   1],\n",
      "        [113, 117, 120, 126, 140,  21,   1, 123],\n",
      "        [124, 125, 109, 111, 140, 134, 114, 112],\n",
      "        [123,   9,   1,  92, 109, 127, 109, 133],\n",
      "        [133, 124, 109, 112, 117,   8,   1, 115],\n",
      "        [123, 112, 117,   1,  92, 117, 119, 123],\n",
      "        [  1, 124, 125, 117, 112, 120, 109, 133],\n",
      "        [109, 116, 109, 120,   1, 119, 122, 140],\n",
      "        [123,   8,   1, 132, 127, 123,   1, 112],\n",
      "        [127, 123, 140, 122, 122, 123,   1, 110],\n",
      "        [114,   1, 124, 125, 123, 126, 122, 128],\n",
      "        [114, 132, 109, 120, 137, 122, 123, 114],\n",
      "        [  1, 126, 109, 121,   1, 124, 125, 117],\n",
      "        [114, 125, 137,   1, 124, 123, 113, 117],\n",
      "        [110, 109, 126, 117, 126, 127, 136, 118],\n",
      "        [123, 122, 117, 121, 109, 127, 137,   1],\n",
      "        [109,   1, 111, 136, 125, 123, 122, 117],\n",
      "        [109, 113, 122, 123,   1, 111,   1, 125],\n",
      "        [112, 123, 111, 123, 125, 117, 120,  20],\n",
      "        [123, 121,   8,   1, 142,   1, 126, 119],\n",
      "        [122, 123, 139,   1, 127, 109, 120, 117],\n",
      "        [113, 137, 139,   1, 124, 123, 113, 124],\n",
      "        [122, 117, 140,   1, 117,   1, 124, 125],\n",
      "        [127, 114, 120, 137,   8,   1, 122, 109],\n",
      "        [114, 120, 137, 126, 127, 111, 109,   8],\n",
      "        [120,   1, 129, 125, 109, 122, 131, 128],\n",
      "        [117, 130, 109, 118, 120, 114,   1,  89],\n",
      "        [  1, 111, 120, 139, 110, 120, 114, 122],\n",
      "        [109, 112, 123, 125, 123, 113, 117, 114],\n",
      "        [111,   1, 132, 120, 114, 122, 136,   9],\n",
      "        [  1,  48, 116, 109, 119, 123, 120, 113],\n",
      "        [125, 109, 116, 123, 126, 120, 109, 122],\n",
      "        [  1, 124, 123, 111, 114, 120,   1, 111],\n",
      "        [  1, 132, 127, 123,   1, 123, 122,   1],\n",
      "        [127, 123,   1, 127, 123,   1, 127, 123],\n",
      "        [132, 109, 120,   8,   1, 111, 136, 130],\n",
      "        [109, 120,   8,   1, 132, 127, 123,   1],\n",
      "        [126, 123, 121,   9,   1,  81, 113, 125],\n",
      "        [119, 123, 125, 136, 126, 127, 122, 123],\n",
      "        [  1, 126, 120, 128, 132, 109, 118, 122],\n",
      "        [  1, 119, 128, 124, 122, 123,   1, 123],\n",
      "        [126, 137,   1, 122, 109,   1, 124, 114],\n",
      "        [  0, 142,   1,  92, 128,   8,   1, 132],\n",
      "        [119, 117, 130, 145,  49,   1, 142,   1],\n",
      "        [123, 120, 123, 115, 114, 122, 122, 136],\n",
      "        [  1, 127, 114, 121,   1, 114, 118,   1]])\n",
      "Target:\n",
      "tensor([[ 71,  55,  68,  68,  55,   8,   1,  48],\n",
      "        [  1, 126,   1, 126, 111, 123, 117, 121],\n",
      "        [  1, 124, 114, 126, 122, 117,   8,   1],\n",
      "        [123, 118,   1, 126,   1, 113, 114, 127],\n",
      "        [114, 122, 122, 136, 121,   1, 124, 128],\n",
      "        [111, 133, 109, 140, 126, 140,   1, 126],\n",
      "        [122, 109, 122, 117, 114,   8,   1, 142],\n",
      "        [140, 111,   1, 117, 116,   1, 133, 119],\n",
      "        [128, 139, 134, 114, 112, 123,   9,   1],\n",
      "        [  1, 110, 136, 120, 123,   1, 114, 121],\n",
      "        [139,   8,   1, 109,   1, 127, 114, 110],\n",
      "        [120, 117,   1, 124, 125, 117, 126, 127],\n",
      "        [116, 128, 110, 136,   8,   1, 133, 124],\n",
      "        [125, 109, 120,   8,   1, 132, 127, 123],\n",
      "        [  1, 119, 127, 123,   1, 120, 123, 133],\n",
      "        [123,   1, 113, 123, 110, 125, 109,   1],\n",
      "        [120, 136, 110, 119, 109,   9,   0, 142],\n",
      "        [126, 127, 137, 139,   9,   1,  93, 122],\n",
      "        [  1, 126, 111, 123, 117, 121,   1, 117],\n",
      "        [117, 120, 126, 140,  21,   1, 123, 122],\n",
      "        [125, 109, 111, 140, 134, 114, 112, 123],\n",
      "        [  9,   1,  92, 109, 127, 109, 133, 109],\n",
      "        [124, 109, 112, 117,   8,   1, 115, 114],\n",
      "        [112, 117,   1,  92, 117, 119, 123, 120],\n",
      "        [124, 125, 117, 112, 120, 109, 133, 114],\n",
      "        [116, 109, 120,   1, 119, 122, 140, 116],\n",
      "        [  8,   1, 132, 127, 123,   1, 112, 123],\n",
      "        [123, 140, 122, 122, 123,   1, 110, 136],\n",
      "        [  1, 124, 125, 123, 126, 122, 128, 120],\n",
      "        [132, 109, 120, 137, 122, 123, 114,   1],\n",
      "        [126, 109, 121,   1, 124, 125, 117, 114],\n",
      "        [125, 137,   1, 124, 123, 113, 117,   1],\n",
      "        [109, 126, 117, 126, 127, 136, 118,   1],\n",
      "        [122, 117, 121, 109, 127, 137,   1,  84],\n",
      "        [  1, 111, 136, 125, 123, 122, 117, 120],\n",
      "        [113, 122, 123,   1, 111,   1, 125, 109],\n",
      "        [123, 111, 123, 125, 117, 120,  20,   1],\n",
      "        [121,   8,   1, 142,   1, 126, 119, 109],\n",
      "        [123, 139,   1, 127, 109, 120, 117, 114],\n",
      "        [137, 139,   1, 124, 123, 113, 124, 120],\n",
      "        [117, 140,   1, 117,   1, 124, 125, 114],\n",
      "        [114, 120, 137,   8,   1, 122, 109, 132],\n",
      "        [120, 137, 126, 127, 111, 109,   8,   1],\n",
      "        [  1, 129, 125, 109, 122, 131, 128, 116],\n",
      "        [130, 109, 118, 120, 114,   1,  89, 128],\n",
      "        [111, 120, 139, 110, 120, 114, 122, 122],\n",
      "        [112, 123, 125, 123, 113, 117, 114,  22],\n",
      "        [  1, 132, 120, 114, 122, 136,   9,   1],\n",
      "        [ 48, 116, 109, 119, 123, 120, 113, 123],\n",
      "        [109, 116, 123, 126, 120, 109, 122, 122],\n",
      "        [124, 123, 111, 114, 120,   1, 111,   1],\n",
      "        [132, 127, 123,   1, 123, 122,   1, 126],\n",
      "        [123,   1, 127, 123,   1, 127, 123, 125],\n",
      "        [109, 120,   8,   1, 111, 136, 130, 123],\n",
      "        [120,   8,   1, 132, 127, 123,   1, 122],\n",
      "        [123, 121,   9,   1,  81, 113, 125, 128],\n",
      "        [123, 125, 136, 126, 127, 122, 123,   1],\n",
      "        [126, 120, 128, 132, 109, 118, 122, 123],\n",
      "        [119, 128, 124, 122, 123,   1, 123, 126],\n",
      "        [137,   1, 122, 109,   1, 124, 114, 125],\n",
      "        [142,   1,  92, 128,   8,   1, 132, 127],\n",
      "        [117, 130, 145,  49,   1, 142,   1, 132],\n",
      "        [120, 123, 115, 114, 122, 122, 136, 118],\n",
      "        [127, 114, 121,   1, 114, 118,   1, 110]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Input:')\n",
    "print(xb)\n",
    "\n",
    "print('Target:')\n",
    "print(yb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EPOCHS_VAL)\n",
    "        for k in range(EPOCHS_VAL):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.key = nn.Linear(EMBED_SIZE, head_size, bias=False)\n",
    "        self.query = nn.Linear(EMBED_SIZE, head_size, bias=False)\n",
    "        self.value = nn.Linear(EMBED_SIZE, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        w = q @ k.transpose(1, 2) * C**-0.5\n",
    "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        v = self.value(x)\n",
    "        out = w @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(EMBED_SIZE, EMBED_SIZE)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(FFN, self).__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size*4, embed_size),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, head_size):\n",
    "        super(Block, self).__init__()\n",
    "        h_size = embed_size // head_size\n",
    "        self.sa = MultiHeadAttention(head_size, h_size)\n",
    "        self.ffn = FFN(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
    "        self.pos_embed = nn.Embedding(BLOCK_SIZE, EMBED_SIZE)\n",
    "        self.blocks = nn.Sequential(*[Block(EMBED_SIZE, HEAD_SIZE) for _ in range(6)])\n",
    "        self.ln = nn.LayerNorm(EMBED_SIZE)\n",
    "        self.lm_head = nn.Linear(EMBED_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        B, T = x.shape\n",
    "        tok_embed = self.token_embed(x)\n",
    "        pos_embed = self.pos_embed(torch.arange(T))\n",
    "        out = tok_embed + pos_embed\n",
    "        out = self.blocks(out)\n",
    "        out = self.ln(out)\n",
    "        out = self.lm_head(out)\n",
    "        \n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = out.shape\n",
    "            out = out.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(out, y)\n",
    "\n",
    "        return out, loss\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) \n",
    "\n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            x_cond = x[:, -block_size:]\n",
    "            out, loss = self(x_cond)\n",
    "            out = out[:, -1, :]\n",
    "            probs = F.softmax(out, dim=-1)\n",
    "            x_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, x_next), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 146])\n",
      "5.212309837341309\n"
     ]
    }
   ],
   "source": [
    "model = LM()\n",
    "out, loss = model(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1682, val loss 4.1386\n",
      "step 500: train loss 2.2335, val loss 2.2131\n",
      "step 1000: train loss 2.0937, val loss 2.0724\n",
      "step 1500: train loss 2.0310, val loss 2.0091\n",
      "step 2000: train loss 1.9790, val loss 1.9622\n",
      "step 2500: train loss 1.9444, val loss 1.9242\n",
      "step 3000: train loss 1.9184, val loss 1.9079\n",
      "step 3500: train loss 1.8948, val loss 1.8877\n",
      "step 4000: train loss 1.8716, val loss 1.8727\n",
      "step 4500: train loss 1.8704, val loss 1.8435\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for epoch in range(EPOCHS):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    out, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_loss:\n",
    "            best_loss = losses['val']\n",
    "            best_model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n«Анна Памений у возвратеин правлость в середнееру моя чомным, офицанить еще унибилье последных била тешать еёвого воздорого, шапь на приближение тоже этом его вам не полкомый положила лошадь все с.\\nИму 1ста читой, поль был,! – сказал государом положения да ИоНоБочных и Лужиках, даже этом делает ему были людовию в домой время, и вывел в когрому, хочется. Князь Андрей. Но маторовник свое увторово знакочить пролки, я взялся этому гляников на мысли уподал крики, пресколь? Сонюшка – Пусгой на ликелуший на была на обм0 разна нает. Княнным и для свое и влеон у ион с седне брестящурь зсорил радоровая в ценелим» этого Соняющие, больь себя частью, чтобы время волчал пододвижно. Причастия военна при деля, что он на не имел на Тишка, что он в знаешего подъезд к да. За читальном что он желая, французский при игратился Пьiра; но воюсь с дочерой стерила радоставляленно ступложил Пьер Успитался, то себя, чтобы соботама?\\n– Я просавлюем похожье? – говорили здесьмо, рукой.\\n– Ну, моя выхороном, и брощал фиже Телии, когда ности получно, нужно представлении уповтрого разрадоаре в своего всё ворозных в одно. Кавало жало Стон, всё вна уладетушь.\\n– Матья буду: Он подомые граф, их Пьера на чешее узаставляя на неомдоманей, обхожой же плоговора, говорила онвратит, так бумаг.\\n– О вот дружевом не более о у держеения тороваясь особерами Берге мог нем и из он восполе была это бы горняя поехо.\\n– Тонной большие ц! – а некоторосил невсего; но привозбий\\nБорбы мнения, в замуже – Поддолго чешал претрогодежения, ; он дарывал неволее дамы и бити трога: торонет вдруг болестящения дете, что бласю ниправлестящика, и вы, возвратах,] – друг;] и нее другачих по оставила и дра. Слову его, – порязов к нужный в комнатой и жизне малерственный всека в я слеза старье за столохо ту зомлянним разбалажение кричала к обойно осбежать ками окну что времением ввечом стоял в яльчого этого для в расскаждым этого французский же лошадь с дражиму он Сион, разда моложила он гибнял штаки жанина, она хочели нескочь ты мелеге ему. Одстрона об он саходя в говорить.\\nБорвших впереда говоров, харои.\\nВении доброго и другу, Надо он поварить был ранок, как и был бы отъехать? рукой горячие, нагновраг и в чать. Два сечей тог'учин, – яжна потому в бы за тому к этому забгорде друг был замуже не головой.\\n– Да обой… и раехал о не не бередчинув краконно опять ее стоие е сячно тамличного, ли учас сидная бало же держал жизньюневшему вна значительни его начее желание, плаговорила наделае на добро ещели и мородоживенным сзнначительно не оборода то то нужно послеими к за Павловна смотрел, на нимет лошадь плетане. За бургск, к – ну, но и груб обвго, опушает я угромиты, виное, – спросило за кодно всё то держать много, что он положиданий, что была святоей.\\nА Багравчен. Вера,  наши шагарнул кроман и подхожал: мать обратился Ростовсего вдурные сиятельно был плечумая, и чужовее первый, и в тепа презрожать что и сзааждом мне хотела свою можно пригелненно вперечо из соозван в молодомжен добростелым, причедним прешех, был и глупа вничто на нетах он отпечал совершенку, влпостановли. Ратом сраво пощем и перволь.\\n«Я в предположивые старые дом, учиттеля чувствие я слыбежалась и воронноннита всредлежатьше речем хорошо! в чель Пьером он душка из поняли наш лагкоду полкал! А фамюей двух па письмо своим отъезда Перком дошля в этого леско да! его впервую голутяно) вы и полежа, что не вядав за кротливо дело. – отверса обеда. – Князь болем и дешек) плео сидел в про улыбансер. Пьер на была то, я вни иселетнинее шенаковорить на брит!, вот к впотому койно вейди, назади, она извесенном болестепей голорожаление бы этой выходилось, и отмость, не молоду о тот ничего не улыбаюсь уним грацузания праватили: не пконять до оцело жизньях в эта выская надо сидело он порубести, которого радатывал еt его же.\\nОн, Денисокой тоже обыло поддарь; она усильния частно появила князь…\\nГорячно была охобык ящения этон с коло подуху встраненно круго с жаркивать или продавальною, и сади страбожения был голос и вообную\\n– И приздесь, что не дежь, сеобяя держгеньем. Закликем помажал с Сони, лестязнат, нападтИвившихся им собену мог и руках по вы достигала дума князся и стоялись вам и с долгов, ещением к волос, графа, блается к бы необкой роборды, знакомых встором не делолицом начащались ее было не мы этом и в гордо!… c'Una равный чувству сына понял к столом в прообростейполительна с Моить в генерал ын огромя зная может тех, уже вперебежала капится в бароз цения стоизи жась; и морестно утгноверился на произжалых французских умогул в корецки, князя Аудержден к армии, Овра были сказывал опятно выпустил, которое здре знать в комнду, а гланиз. – Да ни прозил вуйский мучивленный для этой графу.\\n– Что которых, нее другадо, а – голосами и ображая мал один. Москве на узначения. Очавнеис*я бежаться на этой дело. Обещая гадован, и как и Корзами батвинул его говорю еще ими походствовал.\\nПоднятом взядеть всех Павловника! – старый додъуждала Альба. И своим и за ним видел разговоренное. Москвую Михайлоха. (ожидаясь на очень пничтожив вел столостал, лошадет. С сувезов Ростов, ни и первзгла.\\n– А вами и взглянулся, карье новиж, всё ртах». – Марью, плечил своих; учление, когда мне и услышнокому Богда ёстроениям в вернулся постуда, – отца Девурье очения сдержая вговолос Марья садоОна в гдешником целивый и приходил об ранить, – писань подать.\\nИ егорней скорила его назовтро с жения плохом всегда много… не расспросья волас дитой. Мость на подважение подствие воротна. Сказалы мни фавстру обойдя подни в Шам жальствие словери лицом, востяпляе, не безсподством всё о своим устрроенные внибурол.\\nДолочен. Так тyже, пречал в болеона и ладовое сначить. К одного едокнуло споказывался доволчного дульоф или царского серчил в нем он движу мневискому, бакрикой для Шука, вспомнным помнянула мистом, во возвался был за нешлего время наредив Одуха дело высячной по комнаты, Они как приберели этой позволяен крепко это было жить подки, которай, счастливна был право оэт мира Двин. – Как ах подле что на с наставой, в деременье Замою вылю ук, – отвечал на ранова показывать ему, что об вопрос. Вравным жил над и неждения на быстроим от она порусское, давые, на пойдору в раного не беловью в сестра, и он нельзовался це черных перекрытным усел впечатлово бы с обойдели соражой к непостители, чтобы читали», спутанился. Анна Мог выпавленных был государь». До графиню. А as parre Тrilh… Богла колотом, черком, часть? простоялся смохруженная соверх Фающемую, – как взглянул и ней ходетя сторо упомнет, не волотелоку длы до он запанил время, как свой долго был хотришел он й более, уебя в достеррала груде перьезжая». Сон положенному, подобежая чутьюсь ее не все могла сак Пьер, как от не знако праждащи, толопывал: – покомнал на мое к видя годых слаба, – опять навсего мимое в плохом. Коклонного плаки дворовникали хожидая в с комнатом испомнителя uз скочrли из без порблестеяния ее узнать к впереила к 807 Ах! мижные о я уже к для вечок с Перебох. Я видает этолцу высшего очень трудым и люди разговорилм в смоым поро видели Соня ним огу, не старыставался новолением. – Как ходит вспрособенного отстановил улыбайлых отмороженую имел от выначего к Павло начающие глазами остановил; разговорилась. – И праводошо! уж князь Телит опять девуша. Он неязаомная хорошо, не вашего восноплон, сестей, улых в должен блюстии, чего больше ростоему часы блили сам только пожалу приятную грацами соедов предсеожу это в Борис Сонью рблисьт соходился приганное учлывого у тобхого помежду пойдя ие на первый, в коргу к маленькостью это взяли Пьер служи, котвердитет, что там письмо князь еду по от в думах я. – Так, за мою хлопуная, ожиду говорил Bушка прядете, трада о добревствовала в голородних была видно пролко законнаясь сдущими братот кому, задоро… Куда – говорил вдруг. Вся это смотрел князь на переходилась положу и в эга воистоял Ростово и всё был задрагина. – Да заглядываем сидел это сообгищели себя и знает государяем цубей уж это моста верным вместа мостари во него. Они свои блептяч… в же заложение у ожидал своих. Счастию плеч, Браточных, которевкорах?\\n– И не удавам. Андр«Пьер и крановит ее плучно в Мопрокал гостин, – сказал он чратовлений отом староцев повором; порредко времений торон пошла можу и круг князя Андрея), сестью Бранное в сении. Его к хобы длаза назадраговорят причёски сидел втолясь их она батарил. Бергу исполит опять емно две иехал осида, котороу мог, по хаж в более, когда спуганно? Князь – (Качалесь то ддела в Москве, старала ознакоми ее шагласить! – внучну, у наводухо де голово на непреобрата и тяжел к придеть молодаженицей взговора слова вушем еще. Ей.\\nЖенокил он ещу вздохнула помордорщала брт площал к Болкон?\\n– Только разговариваясь, что Пьер под восходившись у и он от и концузски гостью но было в видно, вместем. – стояла о за для марте, него дложения м ещение ликов. Только взглянулся это? – сказал Долохов; мог был он дошел за отбегн. Это ка, повойска эше его как зон, наши белез к о прегсказал Ррустей. – То воени,] то свои невыправствоварии лошадь подбрестре. Предка назадеми подходя француз задо всё когда стоял левой ему стало лестства пер столляный падисьмая делы Милось, прыганный голосы соовел этот говорила скоро та на радоставяясь ведет, пеле Ростова, и часы участия свое дурны обрававшего желаему насынuний русских получаются, водет вешь.\\nБорил Лицор, начался, кядя цены, а так, груда для кого смотрит, на помнившаюся к – говорить, запутания в одновом прожал, раз были, Ростовых харанные прими», комнор они, увидирова четырертире приштвенный на бала? – занягиням в две, соядывного и, Кавлен… гуской послова, обедоком ж, при как часы перево запуская, и он верочение на за не вольных.\\nГопотому, другрукаенский берлошо, – крикнул княгиня, И кого тепечали же и вместеша, сиделали за хваздо было незумные. От ней.\\n– Грами князана и на не французский девушка. Но командск лужги, гоянникую готова, как ядели до боивленно был к батать этом образах Цухой модвижения пожало на почавить гомнал со мне многии во в дажго, бавсего и детьжен беребкорала ходите, что все просил при этом так Не говорился дете назадно быстоять счастливоспышенного лес\""
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_text = decode(model.generate(x, max_new_tokens=10000)[0].tolist())\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_text.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'war_and_peace_transformer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
